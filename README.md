# LexiGPT ‚Äì Agentic Legal Co-Pilot

LexiGPT is an offline-first legal assistant that combines a planner‚Äìexecutor agent loop, Retrieval-Augmented Generation (RAG), and document drafting utilities on top of a local Ollama deployment. The stack pairs a lightweight Flask API with a responsive HTML/JS client, and runs entirely on-device for privacy-conscious workflows.

## Core Capabilities

- **Conversational Legal Assistant** ‚Äì multi-turn chat backed by Ollama (e.g., `llama3`) with persisted local history.
- **RAG Pipeline** ‚Äì ChromaDB vector store bootstrapped from `data/combined.json` for grounded answers with inline citations.
- **Agentic Loop** ‚Äì planner ‚Üí executor ‚Üí evaluator chain that can call tools such as file readers, regex extractors, RAG search, and document generation.
- **Document Drafting** ‚Äì ready-to-use templates (NDA, employment offers, legal notices) rendered locally with structured inputs.
- **Offline + Private** ‚Äì no cloud calls; all data, embeddings, and chat transcripts stay on disk.

## Project Structure

```
‚îú‚îÄ‚îÄ app.py                  # Flask application factory
‚îú‚îÄ‚îÄ routes/                 # API blueprints (chat, RAG, agent, docgen, history)
‚îú‚îÄ‚îÄ services/               # Ollama client, agent runtime, docgen, chat store
‚îú‚îÄ‚îÄ rag/                    # Vector DB + retriever helpers
‚îú‚îÄ‚îÄ data/combined.json      # Seed legal corpus for Chroma
‚îú‚îÄ‚îÄ index.html / script.js  # Front-end chat interface
‚îî‚îÄ‚îÄ requirements.txt
```

## Getting Started

### Prerequisites

- Python 3.10+
- [Ollama](https://ollama.com) with a local model pulled (e.g., `ollama pull llama3`)
- `chromadb` dependencies (installed via `pip`)

### Setup

1. **Install dependencies**
   ```bash
   python -m venv .venv
   .\.venv\Scripts\activate
   pip install -r requirements.txt
   ```
2. **(Optional) Seed/verify the Chroma DB**
   ```bash
   python rag/chroma_init.py
   ```
   The retriever will auto-seed from `data/combined.json` if the persistent store is empty.
3. **Run the Flask API**
   ```bash
   python app.py
   ```
4. **Open the UI**
   - Serve `index.html` via any static server **or**
   - Double-click it and let the scripts speak to `http://localhost:5000` (CORS is enabled).

## API Highlights

| Endpoint | Method | Description |
| --- | --- | --- |
| `/api/chat` | POST | Main chat endpoint. Pass `{ message, session_id?, mode? }`; returns `{ response, session_id, context? }`. `mode="rag"` forces contextual answers. |
| `/api/rag-query` | POST | Direct access to the RAG pipeline. |
| `/api/chats` | GET/POST | List or create chat sessions. |
| `/api/chats/<id>` | GET | Retrieve a session (messages, title, timestamps). |
| `/api/agent/plan-run` | POST | Kicks off the planner ‚Üí executor ‚Üí evaluator loop with `{ goal }`. |
| `/api/docgen` | POST | Generate documents with `{ template, fields }`. Templates: `nda`, `employment_offer`, `legal_notice`. |

## Configuration

Environment variables (optional) in `config.py`:

| Variable | Default | Purpose |
| --- | --- | --- |
| `OLLAMA_MODEL` | `llama3` | Model to run via Ollama |
| `OLLAMA_HOST` | `http://localhost:11434` | Ollama HTTP endpoint |
| `VECTOR_DB_DIR` | `rag/vectordb` | Persistent Chroma folder |
| `LEGAL_DATA_FILE` | `data/combined.json` | Seed corpus |
| `CHAT_HISTORY_FILE` | `data/chat_history.json` | Chat transcript store |

## Testing the Flow

1. **Regular Chat** ‚Äì type any legal query; the backend auto-creates a session and persists it locally.
2. **RAG Mode** ‚Äì call the `/api/chat` endpoint with `{"mode": "rag"}` to ensure the answer is grounded in the retrieved context.
3. **Document Drafts** ‚Äì `POST /api/docgen`:
   ```json
   {
     "template": "nda",
     "fields": {
       "disclosing_party": "Acme Ltd.",
       "receiving_party": "Spark Labs",
       "purpose": "sharing product roadmap",
       "term": "24 months",
       "governing_law": "Laws of India"
     }
   }
   ```
4. **Planner Loop** ‚Äì `POST /api/agent/plan-run` with `{ "goal": "Summarise Section 3 of the Motor Vehicles Act" }`.

## Troubleshooting

- **Ollama not reachable** ‚Äì ensure `ollama serve` is running and the `OLLAMA_HOST` matches.
- **Empty RAG results** ‚Äì check that `rag/vectordb` contains the persisted Chroma files or rerun `rag/chroma_init.py`.
- **Permission issues** ‚Äì Windows users may need to run the terminal as Administrator when initializing Chroma for the first time.

Happy lawyering! üßë‚Äç‚öñÔ∏èüíª
